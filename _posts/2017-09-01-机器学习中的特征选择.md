---
layout:     post
title:      "机器学习中的特征选择"
subtitle:   " \"数据预处理的关键过程\""
date:       2017-09-01 12:00:00
author:     "Nova"
header-img: "img/post/post-bg-rainbow.jpg"
catalog: true
tags:
    - 机器学习
    - 特征选择
---

## 1 特征选择的问题概念

​	我们能用很多属性描述一个客观世界中的对象，例如对于描述一个人来说，可以获取到身高、体重、年龄、性别、学历、收入等等，但对于评判一个人的信用级别来说，往往只需要获取他的年龄、学历、收入这些信息。换言之，对一个学习任务来说，给定属性集，其中有些属性可能很关键、很有用，另一些属性则可能没什么用。我们将属性称为“特征” (feature)，对当前学习任务有用的属性称为“相关特征” (relevant feature)、没什么用的属性称为“无关特征” (irrelevant feature)。从给定的特征集合中选择出相关特征子集的过程，称为**“特征选择” (feature selection)**。

​	特征选择是一个重要的“数据预处理” (data preprocessing) 过程，在现实机器学习任务中，获得数据之后通常先进行特征选择，此后再训练学习器。那么，为什么要进行特征选择呢？

​	有两个很重要的原因：

- 我们在现实任务中经常会遇到维数灾难问题，这是由于属性过多造成的，若能从中选择出重要的特征，使得后续学习过程仅需在一部分特征上构建模型，则维数灾难问题会大为减轻。
- 去除不相关特征往往会降低学习任务的难度，这就像侦探破案一样，若将纷繁复杂的因素抽丝剥茧，只留下关键因素，则真相往往更易看清。

  ​需要注意的是，特征选择过程必须确保不丢失重要特征，否则后续学习过程会因为重要信息的缺失而无法获得更好的性能。给定数据集，若学习任务不同，则相关特征很可能不同，因此，特征选择中所谓的“无关特征”是指与当前学习任务无关。有一类特征称为“冗余特征” (redundant feature)，它们所包含的信息能从其他特征中推演出来。例如，考虑立方体对象，若已有特征“底面长”和“底面宽”，则“底面积”是冗余特征，因为它能从前二者得到。冗余特征在很多时候不起作用，去除它们会减轻学习过程的负担。但有时冗余特征会降低学习任务的难度，例如若学习目标是估算立方体的体积，则“底面积”这个冗余特征的存在将使得体积的估算更容易；更确切地说，若某个冗余特征恰好对应了完成学习任务所需的“中间概念”，则该冗余特征是有益的。

## 2 子集搜索与评价

​	欲从初始的特征集合中选取一个包含了所有重要信息的特征子集，若没有任何领域知识作为先验假设，那就只好遍历所有可能的子集了；然而这在计算上却是不可行的，因为这样做会遭遇组合爆炸，特征个数稍多就无法进行。可行的做法是产生一个“候选子集”，评价出它的好坏，基于评价结果产生下一个候选子集，在对其进行评价，…… 这个过程持续进行下去，直至无法找到更好的候选子集为止。显然，这里涉及两个关键环节：如何根据评价结果获取下一个候选特征子集？如何评价候选特征子集的好坏？

​	第一个环节是“子集搜索” (subset search) 问题。给定特征集合 $$\{a_1, a_2, \dots , a_d\}$$，我们可将每个特征看作一个候选子集，对这 $$d$$ 个候选单特征子集进行评价，假定 $$\{ a_2 \}$$ 最优，于是将 $$\{ a_2 \}$$ 作为第一轮的选定集；然后，在上一轮的选定集中加入一个特征，构成包含两个特征的候选子集，假定在这 $$d-1$$ 个候选两特征子集中 $$\{ a_2, a_4\}$$ 最优，且优于 $$\{ a_2 \}$$，于是将 $$\{ a_2, a_4\}$$ 作为本轮的选定集；…… 假定在第 $$k+1$$ 轮时，最优的候选 $$(k+1)$$ 特征子集不如上一轮的选定集，则停止生成候选子集，并将上一轮选定的 $$k$$ 特征集合作为特征选择结果。**这样逐渐增加相关特征的策略称为“前向” (forward) 搜索**。类似的，若我们从完整的特征集合开始，每次尝试去掉一个无关特征，这样逐渐减少特征的策略称为“后向” (backward) 搜索。还可将前向与后向搜索结合起来，每一轮逐渐增加选定相关特征 (这些特征在后续轮中将确定不会被去除)、同时减少无关特征，这样的策略称为“双向” (bidirectional) 搜索。

​	显然，上述策略都是贪心的，因为它们仅考虑了使本轮选定集最优，例如在第三轮假定选择 $$a_5$$ 优于 $$a_6$$，于是选定集为 $$\{ a_2, a_4, a_5\}$$，然而在第四轮中却可能是 $$\{ a_2, a_4, a_6, a_8\}$$ 比所有的 $$\{ a_2, a_4, a_5, a_i\}$$ 都更优。遗憾的是，若不进行穷举搜索，则这样的问题无法避免。

​	第二个环节是“子集评价” (subset evaluation) 问题。给定数据集 $$D$$，假定 $$D$$ 中第 $$i$$ 类样本所占的比例为 $$p_i (i=1,2,\dots, \mid \mathcal{Y} \mid)$$。为便于讨论，假定样本属性均为离散型。对于属性子集 $$A$$，假定根据其取值将 $$D$$ 分成了 $$V$$ 个子集 $$\{ D^1,D^1,\dots,D^V \}$$，每个子集中的样本在 $$A$$ 上取值相同，于是我们可计算属性子集 $$A$$ 的信息增益：

​	$$\displaystyle{ Gain(D,a) = Ent(D) - \sum_{v=1}^{V} \frac{\mid D^v \mid}{\mid D \mid} Ent(D^v) }$$

其中信息熵定义为：

​	$$\displaystyle{ Ent(D) = -\sum_{k=1}^{\mid \mathcal{Y} \mid} p_k\log_2{p_k} }$$

信息增益越大，意味着特征子集 $$A$$ 包含的有助于分类的信息越多。于是，对每个候选特征子集，我们可基于训练数据集 $$D$$ 来计算其信息增益，以此作为评价准则。

​	更一般的，特征子集 $$A$$ 实际上确定了对数据集 $$D$$ 的一个划分，每个划分区域对应着 $$A$$ 上的一个取值，而样本标记信息 $$Y$$ 则对应着对 $$D$$ 的真是划分，通过估算这两个划分的差异，就能对 $$A$$ 进行评价。与 $$Y$$ 对应的划分的差异越小，则说明 $$A$$ 越好。信息熵仅是判断这个差异的一种途径，其他能判断两个划分差异的机制都能用于特征子集评价。

​	将特征子集搜索机制与子集评价机制相结合，即可得到特征选择方法。例如将前向搜索与信息熵结合，这显然与决策树算法非常相似。事实上，决策树可用于特征选择，树节点的划分属性所组成的集合就是选择出的特征子集。其他的特征选择方法未必像决策树特征选择这么明显，但它们在本质上都是显式或隐式地结合了某种 (或多种) 子集搜索机制和子集评价机制。

​	**常见的特征选择方法大致可分为三类：过滤式 (filter)、包裹式 (wrapper) 和嵌入式 (embedding)。**

