---
layout:     post
title:      "机器学习中的特征选择"
subtitle:   " \"数据预处理的关键过程\""
date:       2017-09-01 12:00:00
author:     "Nova"
header-img: "img/post/post-bg-rainbow.jpg"
catalog: true
tags:
    - 机器学习
    - 特征选择
---

## 1 特征选择问题的概念

​	我们能用很多属性描述一个客观世界中的对象，例如对于描述一个人来说，可以获取到身高、体重、年龄、性别、学历、收入等等，但对于评判一个人的信用级别来说，往往只需要获取他的年龄、学历、收入这些信息。换言之，对一个学习任务来说，给定属性集，其中有些属性可能很关键、很有用，另一些属性则可能没什么用。我们将属性称为“特征” (feature)，对当前学习任务有用的属性称为“相关特征” (relevant feature)、没什么用的属性称为“无关特征” (irrelevant feature)。从给定的特征集合中选择出相关特征子集的过程，称为**“特征选择” (feature selection)**。

​	特征选择是一个重要的“数据预处理” (data preprocessing) 过程，在现实机器学习任务中，获得数据之后通常先进行特征选择，此后再训练学习器。那么，为什么要进行特征选择呢？

​	有两个很重要的原因：

- 我们在现实任务中经常会遇到维数灾难问题，这是由于属性过多造成的，若能从中选择出重要的特征，使得后续学习过程仅需在一部分特征上构建模型，则维数灾难问题会大为减轻。
- 去除不相关特征往往会降低学习任务的难度，这就像侦探破案一样，若将纷繁复杂的因素抽丝剥茧，只留下关键因素，则真相往往更易看清。

  ​需要注意的是，特征选择过程必须确保不丢失重要特征，否则后续学习过程会因为重要信息的缺失而无法获得更好的性能。给定数据集，若学习任务不同，则相关特征很可能不同，因此，特征选择中所谓的“无关特征”是指与当前学习任务无关。有一类特征称为“冗余特征” (redundant feature)，它们所包含的信息能从其他特征中推演出来。例如，考虑立方体对象，若已有特征“底面长”和“底面宽”，则“底面积”是冗余特征，因为它能从前二者得到。冗余特征在很多时候不起作用，去除它们会减轻学习过程的负担。但有时冗余特征会降低学习任务的难度，例如若学习目标是估算立方体的体积，则“底面积”这个冗余特征的存在将使得体积的估算更容易；更确切地说，若某个冗余特征恰好对应了完成学习任务所需的“中间概念”，则该冗余特征是有益的。

## 2 子集搜索与评价

​	欲从初始的特征集合中选取一个包含了所有重要信息的特征子集，若没有任何领域知识作为先验假设，那就只好遍历所有可能的子集了；然而这在计算上却是不可行的，因为这样做会遭遇组合爆炸，特征个数稍多就无法进行。可行的做法是产生一个“候选子集”，评价出它的好坏，基于评价结果产生下一个候选子集，在对其进行评价，…… 这个过程持续进行下去，直至无法找到更好的候选子集为止。显然，这里涉及两个关键环节：如何根据评价结果获取下一个候选特征子集？如何评价候选特征子集的好坏？

​	第一个环节是“子集搜索” (subset search) 问题。给定特征集合 $$\{a_1, a_2, \dots , a_d\}$$，我们可将每个特征看作一个候选子集，对这 $$d$$ 个候选单特征子集进行评价，假定 $$\{ a_2 \}$$ 最优，于是将 $$\{ a_2 \}$$ 作为第一轮的选定集；然后，在上一轮的选定集中加入一个特征，构成包含两个特征的候选子集，假定在这 $$d-1$$ 个候选两特征子集中 $$\{ a_2, a_4\}$$ 最优，且优于 $$\{ a_2 \}$$，于是将 $$\{ a_2, a_4\}$$ 作为本轮的选定集；…… 假定在第 $$k+1$$ 轮时，最优的候选 $$(k+1)$$ 特征子集不如上一轮的选定集，则停止生成候选子集，并将上一轮选定的 $$k$$ 特征集合作为特征选择结果。**这样逐渐增加相关特征的策略称为“前向” (forward) 搜索**。类似的，若我们从完整的特征集合开始，每次尝试去掉一个无关特征，这样逐渐减少特征的策略称为“后向” (backward) 搜索。还可将前向与后向搜索结合起来，每一轮逐渐增加选定相关特征 (这些特征在后续轮中将确定不会被去除)、同时减少无关特征，这样的策略称为“双向” (bidirectional) 搜索。

​	显然，上述策略都是贪心的，因为它们仅考虑了使本轮选定集最优，例如在第三轮假定选择 $$a_5$$ 优于 $$a_6$$，于是选定集为 $$\{ a_2, a_4, a_5\}$$，然而在第四轮中却可能是 $$\{ a_2, a_4, a_6, a_8\}$$ 比所有的 $$\{ a_2, a_4, a_5, a_i\}$$ 都更优。遗憾的是，若不进行穷举搜索，则这样的问题无法避免。

​	第二个环节是“子集评价” (subset evaluation) 问题。给定数据集 $$D$$，假定 $$D$$ 中第 $$i$$ 类样本所占的比例为 $$p_i (i=1,2,\dots, \mid \mathcal{Y} \mid)$$。为便于讨论，假定样本属性均为离散型。对于属性子集 $$A$$，假定根据其取值将 $$D$$ 分成了 $$V$$ 个子集 $$\{ D^1,D^1,\dots,D^V \}$$，每个子集中的样本在 $$A$$ 上取值相同，于是我们可计算属性子集 $$A$$ 的信息增益：

​	$$\displaystyle{ Gain(D,a) = Ent(D) - \sum_{v=1}^{V} \frac{\mid D^v \mid}{\mid D \mid} Ent(D^v) }$$

其中信息熵定义为：

​	$$\displaystyle{ Ent(D) = -\sum_{k=1}^{\mid \mathcal{Y} \mid} p_k\log_2{p_k} }$$

信息增益越大，意味着特征子集 $$A$$ 包含的有助于分类的信息越多。于是，对每个候选特征子集，我们可基于训练数据集 $$D$$ 来计算其信息增益，以此作为评价准则。

​	更一般的，特征子集 $$A$$ 实际上确定了对数据集 $$D$$ 的一个划分，每个划分区域对应着 $$A$$ 上的一个取值，而样本标记信息 $$Y$$ 则对应着对 $$D$$ 的真是划分，通过估算这两个划分的差异，就能对 $$A$$ 进行评价。与 $$Y$$ 对应的划分的差异越小，则说明 $$A$$ 越好。信息熵仅是判断这个差异的一种途径，其他能判断两个划分差异的机制都能用于特征子集评价。

​	将特征子集搜索机制与子集评价机制相结合，即可得到特征选择方法。例如将前向搜索与信息熵结合，这显然与决策树算法非常相似。事实上，决策树可用于特征选择，树节点的划分属性所组成的集合就是选择出的特征子集。其他的特征选择方法未必像决策树特征选择这么明显，但它们在本质上都是显式或隐式地结合了某种 (或多种) 子集搜索机制和子集评价机制。

​	**常见的特征选择方法大致可分为三类：过滤式 (filter)、包裹式 (wrapper) 和嵌入式 (embedding)。**

## 3 过滤式选择

​	过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关。这相当于先用特征选择过程对初始特征进行“过滤”，再用过滤后的特征来训练模型。

​	Relief (Relevant Features) [Kira and Rendell, 1992] 是一种著名的过滤式特征选择方法，该方法设计了一个“相关统计量”来度量特征的重要性。该统计量是一个 向量，其每个分量分别对应于一个初始特征，而特征子集的重要性则是由子集中每个特征所对应的相关统计量分量之和来决定。于是，最终只需指定一个阈值 $$\tau$$，然后选择比 $$\tau$$ 大的相关统计量分量所对应的特征即可；也可以指定欲选取的特征个数 $$k$$，然后选择相关统计量分量最大的 $$k$$ 个特征。

​	显然，Relief 的关键是如何确定相关统计量。给定训练集 $$\{ (x_1, y_1), (x_2, y_2), \dots, (x_m, y_m) \}$$，对每个示例 $$x_i$$，Relief 先在 $$x_i$$ 的同类样本中寻找其最邻近 $$x_{i,nh}$$，称为“猜中邻近” (near-hit)，在从 $$x_i$$ 的异类样本中寻找其最近邻 $$x_{i,nm}$$，称为“猜错邻近” (near-miss)，然后，想干统计量对应于属性 $$j$$ 的分量为：

​	$$\delta^j = \sum_{i} -{\rm diff} (x_i^j, x_{i,nh}^j)^2 + {\rm diff} (x_i^j, x_{i,nm}^j)^2$$

其中 $$x_a^j$$ 表示样本 $$x_a$$ 在属性 $$j$$ 上的取值，$${\rm diff} (x_a^j, x_{b}^j)$$ 取决于属性 $$j$$ 的类型：若属性 $$j$$ 为离散型，则 $$x_a^j = x_{b}^j$$ 时 $${\rm diff} (x_a^j, x_{b}^j) = 0$$，否则为 1；若属性 $$j$$ 为连续型，则 $${\rm diff} (x_a^j, x_{b}^j) = \mid x_a^j - x_b^j \mid$$，注意 $$x_a^j$$，$$x_b^j$$ 已规范化到 $$[0, 1]$$ 区间。

​	从上式可看出，若样本与其猜中近邻在某一属性上的距离小于该样本与其猜错近邻的距离，则说明这一属性对区分同类与异类样本是有益的，于是增大该属性对应的统计量分量；反之，则说明该属性起负面作用，于是减小其对应的统计量分量。最后，对基于不同样本得到的估计结果进行平均，就得到各属性的相关统计量分量，分量值越大，则对应属性的分类能力就越强。

## 4 包裹式选择

​	与过滤式特征选择不考虑后续学习器不同，包裹式特征选择直接把最终将要使用的学习器的性能作为特征子集的评价准则。换言之，包裹式特征选择的目的就是为给定学习器选择最有利于其性能、“量身定做”的特征子集。

​	一般而言，由于包裹式特征选择方法直接针对给定学习器进行优化，因此从最终学习器性能来看，包裹式特征选择比过滤式特征选择更好，但另一方面，由于在特征选择过程中需多次训练学习器，因此包裹式特征选择的计算开销通常比过滤式特征选择大得多。

​	LVM (Las Vegas Wrapper) [Liu and Setiono, 1996] 是一个典型的包裹式特征选择方法。它在拉斯维加斯方法 (Las Vegas method) 框架下使用随机策略来进行子集搜索，并以最终分类器的误差为特征子集评价准则。该算法是通过在数据集 $$D$$ 上，使用交叉验证法来估计学习器 $$\mathcal{L}$$ 的误差，注意这个误差是在仅考虑特征子集 $$A'$$ 时得到的，即特征子集 $$A'$$ 中包含的特征数更少，则将 $$A'$$ 保留下来。

​	需注意的是，由于 LVW 算法中特征子集搜索采用了随机策略，而每次特征子集评价都需要训练学习器，计算开销很大，因此算法设置了停止条件控制参数 $$T$$。然而，整个 LVM 算法是基于拉斯维加斯方法框架，若初始特征数很多(即 $$\mid A \mid$$ 很大) 、$$T$$ 设置较大，则算法可能运行很长时间都达不到停止条件。换言之，若有运行时间限制，则有可能给不出解。