---
layout:     post
title:      "贝叶斯推断方法"
subtitle:   " \"从经验知识到推断未知\""
date:       2017-02-08 22:00:00
author:     "Nova"
header-img: "img/post/post-bg-3.jpg"
catalog: true
tags:
    - 机器学习
    - 贝叶斯
---


## 1 什么是贝叶斯

​	在机器学习领域，通常将监督学习 (supervised learning) 划分为两大类方法：**生成模型 (generative model)** 与**判别模型 (discriminative model)**，贝叶斯方法正是生成模型的代表 (还有隐马尔科夫模型)。在概率论与统计学中，贝叶斯定理 (Bayes' theorem) 或称为贝叶斯法则 (Bayes' law or Bayes' rule) 表达了一个事件发生的概率，而确定这一概率的方法是基于与该事件相关的条件先验知识 (prior knowledge)。例如，如果患癌症是与人的年龄相关的，那么使用贝叶斯方法，我们可以利用患癌症人群的年龄分布这个先验知识评判一个人患癌症的概率，这相比于不利用年龄信息去判断一个人是否患癌症会聪明得多。可见，上述过程是贝叶斯定理的一种实际应用，通常我们成之为：**贝叶斯推断 (Bayesian inference)**。

## 2 贝叶斯定理

​	开始谈贝叶斯定理之前，必须首先从条件概率 (conditional probability) 说起。所谓条件概率，就是在一个事件发生的情况下，去判断另一个相关联的事件发生的概率，或者简单说，就是指在事件 B 发生的情况下，事件 A 发生的概率。通常记为 $$P(A \mid B)​$$。接下来对贝叶斯公式做一个简单的推导，根据概率知识，我们可以求得 $$P(A \mid B)​$$ 为：

​	$$\displaystyle{P(A \mid B)=\frac{P(A \cap B)}{P(B)}}$$

同样的，有些统计学家更倾向于将其作为一个概率公理 (as an axiom of probability)，记为：

​	$$P(A \cap B)=P(A \mid B) P(B)$$

​	$$P(A \cap B)=P(B \mid A) P(A)$$

因此，可以得到：

​	$$P(A \mid B)P(B)=P(B \mid A)P(A)$$

由此，推导出了大名鼎鼎的贝叶斯公式：

​	$$\displaystyle{ P(A \mid B)=\frac{P(B \mid A)P(A)}{P(B)}}$$

这也是条件概率的计算公式。

## 3 一个小例子 (狗与盗窃)

​	住在一座别墅里的一家人，在过去的 20 年中，发生了 2 次盗窃，这家人养了一只狗，这只狗平均每周晚上叫 3 次，而且，当发生盗窃时，这只狗会叫的概率是 0.9，那么问题是：在这只狗叫的情况下，发生盗窃的概率是多少？

​	我们假设 A 事件是狗晚上叫，则 $$P(A)=\frac{3}{7}$$，假设 B 事件是发生盗窃，则 $$P(B)=\frac{2}{20*365}=\frac{1}{3650}$$，	我们还知道，当 B 事件发生的条件下，A 事件发生的概率 $$P(A \mid B)=0.9$$。因此，根据贝叶斯公式，我们推断得到，狗叫时，发生盗窃的概率，即 $$P(B \mid A)$$：

​	$$\displaystyle {P(B \mid A)=\frac{P(A \mid B)P(B)}{P(A)}=\frac{0.9 \times \frac{1}{3650} }{\frac{3}{7}} \approx 0.00058 }$$

## 4 朴素贝叶斯法解决分类问题

​	根据上述的基本介绍与例子的引入，对贝叶斯定理有了初步的认识。那么接下来的重点就在于，如何根据这一定理，去解决机器学习中的分类问题。

### 4.1 基本方法

​	与所有机器学习问题的基本框架一样，假设我们有输入的特征向量 $$X$$，输出的类别标记 (class label) $$Y$$。$$P(X,Y)$$ 是 $$X$$ 和 $$Y$$ 的联合概率分布。训练数据集为：

​	$$T=\{ (x_1, y_1), (x_2, y_2), \cdots ,(x_n, y_n) \}$$

由 $$P(X,Y)$$ 独立同分布产生。

​	朴素贝叶斯方法通过训练数据集学习这个联合概率分布$$P(X,Y)$$。具体而言，它需要学习两个部分，第一是先验概率分布：

​	$$P(Y=c_k), \; k=1,2, \cdots, K$$

第二是条件概率分布：

​	$$P(X=x \mid Y=c_k) = P(X^{(1)}=x_1, \dots, X^{(n)}=x^{(n)} \mid Y=c_k),\; k=1,2,\cdots,K$$

(注意：这里 $$X$$ 的上标 $$^{(n)}$$ 是表示输入的维度，总共有 $$n$$ 维)。

​	这里，对于条件概率  $$P(x \mid c)$$ 来说，由于它涉及关于 $$x$$ 所有属性的联合概率，有指数级数量的参数，直接 根据样本出现的频率来估计将会遇到严重的困难。例如，假设样本包含 d 个属性且都是二值的，则样本空间将有 $$2^d$$ 种可能的取值，在现实应用中国，这个值往往远大于训练样本数量，也就是说，很多样本取值在训练集中根本没有出现，直接使用频率来估计 $$P(x \mid c)$$ 显然不可行，因为 “未被观测到” 与 “出现概率为零” 通常是不同的。



