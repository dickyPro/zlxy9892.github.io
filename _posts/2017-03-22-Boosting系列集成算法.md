---
layout:     post
title:      "Boosting系列集成学习算法"
subtitle:   " \"序列提升集成学习的威力\""
date:       2017-03-22 12:00:00
author:     "Nova"
header-img: "img/post/post-bg-2.jpg"
catalog: true
tags:
    - 机器学习
    - 集成学习
    - boosting
---

### 1 AdaBoost

#### 1.1 AdaBoost 算法思路

算法的设计流程：

设 $$u^{(t)}$$ 为第 $$t$$ 步时每个样本的权重向量，其初始化值可表达如下：

$$u^{(1)} = [\frac{1}{N}, \frac{1}{N}, \cdots,\frac{1}{N}]$$

for $$t=1,2,\dots,T$$

1. 根据预设好的算法 $$\mathcal{A}$$ （如决策树桩 decision stump），依据 $$\mathcal{A}(\mathcal{D},u^{(t)})$$ 得到 $$g_t$$，算法 $$\mathcal{A}$$ 的目标是得到在权重 $$u^{(t)}$$ 基础上的最小训练误差 error.

2. 更新 $$u^{(t)}$$ 到 $$u^{(t+1)}$$，更新规则如下：

   计算 $$\epsilon_{t} = P_{x \sim \mathcal{D}} \left(y \neq g_t(x)\right) = \frac{\sum_{n=1}^N u_n^{(t)} [ y_n \neq g_t(x_n)]}{\sum_{n=1}^Nu_n^{(t)}}$$

   计算 $$\Delta t = \sqrt[]{\frac{1-\epsilon_t}{\epsilon_t}}$$

   若 $$y_n \neq g_t(x_n)$$ （错误样本）：$$u_n^{t+1} = u_n^{t} \times \Delta t$$

   若 $$y_n = g_t(x_n)$$ （正确样本）：$$u_n^{t+1} = u_n^{t} / \Delta t$$

3. 计算 $$\alpha_t = \ln(\Delta t)$$

最终，得到集成的模型：$$G(x)=sign \left( \sum_{t=1}^T \alpha_t g_t(x) \right)$$

不难发现，在该算法中有两个重要的计算值：$$\Delta t$$ 和 $$\alpha_t$$，当 $$\epsilon_t$$ 越小时，$$\Delta t$$ 会越大，若 $$\epsilon_t = 0$$，也就是分类器完全正确，那么 $$\Delta t$$ 将会是正无穷 $$\infty$$，显然，这样的设计是合理的。再看 $$\alpha_t$$，若 $$\Delta t = 1$$，此时 $$\epsilon_t = 0.5$$ （此时的分类器等效于完全随机猜测的分类精度），那么 $$\alpha_t = 0$$，该分类器在最终模型中的权重为0，也就是说，$$\alpha_t $$ 的大小对应于分类器的精度，显然，这样的设计也是非常合理的。

#### 1.2 AdaBoost 深入理解

在上述算法循环体内的步骤2中，$$u^{(t+1)}$$ 的更新值可以用一个统一的表达式来计算：

$$u_n^{t+1} = u_n^{t} \times \Delta t^{-y_n g_t(x_n)} = u_n^{(t)}\exp(-\alpha_t y_n g_t(x_n))$$

即：$$y_n$$ 与 $$g_t(x_n)$$ 异号，则 $$u_n^{t+1} = u_n^{t} \times \Delta t$$，$$y_n$$ 与 $$g_t(x_n)$$ 同号，则 $$u_n^{t+1} = u_n^{t} / \Delta t$$

同时，还可将 $$u_n^{T+1}$$ 的计算公式写成如下形式：

$$u_n^{(T+1)} = u_n^{(1)} \cdot \prod_{t=1}^{T} \exp{\left( -y_n \alpha_t g_t(x_n) \right)}$$

$$u_n^{(T+1)} = \frac{1}{N} \cdot \exp \left( -y_n {\sum_{t=1}^T \alpha_t g_t(x_n)} \right)$$

对照之前得到的最终模型的公式：$$G(x)=sign \left( \sum_{t=1}^T \alpha_t g_t(x) \right)$$，如果把 $$\sum_{t=1}^T \alpha_t g_t(x)$$ 看做模型推测的投票分数 (voting score) ，可以发现在 AdaBoost 算法中：$$u_n^{(T+1)} \propto \exp \left( -y_n \space ('voting\_score\_on \space x_n \space ') \right)$$

现在，继续看 voting score 这个公式：$$\sum_{t=1}^T \alpha_t g_t(x)$$ 看成是一种线性组合的话，即类似于：$$w_i \cdot \phi_i(x_n)$$ 的形式。那么此时，我们可大胆联想一下 SVM 中的间隔的计算方式：SVM margin = $$\frac{y_n\left( w^T x_n + b \right)}{\| w \|}$$.

其实，$$y_n \space ('voting\_score\_on \space x_n \space ')$$ 相当于是一种有符号，没有标准化的间隔距离，对于分类问题来说，我们都希望这种间隔距离是正值且越大越好。那么对于 $$u_n^{(T+1)}$$ 来说，就希望该值越小越好。这里我们还能够引申出一个论断：在 AdaBoost 中，$$\sum_{i=1}^{N}u_i^{(t)}$$ 是随着迭代次数 $$t$$ 单调递减的（这里不做进一步证明，但直觉上是 make sense 的）。

