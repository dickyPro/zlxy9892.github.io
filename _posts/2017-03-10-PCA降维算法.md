---
layout:     post
title:      "PCA 降维算法"
subtitle:   " \"PCA 算法原理与实现\""
date:       2017-03-01 12:00:00
author:     "Nova"
header-img: "img/post/post-bg-rainbow.jpg"
catalog: true
tags:
    - 机器学习
    - PCA
    - 降维
---

## 1 简介

PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA 通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。

## 2 算法思路

简述一下 PCA 的算法步骤：

设有 $$n$$ 条 $$d$$ 维数据。

1. 将原始数据按列组成 $$n$$ 行 $$d$$ 列矩阵 $$X$$
2. 将 $$X$$ 的每一列（代表一个属性）进行零均值化，即减去这一列的均值
3. 求出协方差矩阵 $$C=\frac{1}{m}XX^\mathsf{T}$$
4. 求出协方差矩阵的特征值及对应的特征向量
5. 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 $$k$$ 行组成矩阵 $$P$$
6. $$Y=PX$$ 即为降维到 $$k$$ 维后的数据

## 3 实现代码

以下是基于 python 的 numpy 库实现的 PCA 算法代码，实现数据降维的功能。

[源代码 GitHub 地址](https://github.com/zlxy9892/ml_code/tree/master/algorithm/pca)

```python
# -*- coding: utf-8 -*-

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


def loadDataSet(filename):
    df = pd.read_table(filename, sep='\t')
    return np.array(df)

def showData(dataMat, reconMat):
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(dataMat[:, 0], dataMat[:, 1], c='green')
    ax.scatter(np.array(reconMat[:, 0]), reconMat[:, 1], c='red')
    plt.show()

def pca(dataMat, topNfeat=999999):

    # 1.对所有样本进行中心化（所有样本属性减去属性的平均值）
    meanVals = np.mean(dataMat, axis=0)
    meanRemoved = dataMat - meanVals

    # 2.计算样本的协方差矩阵 XXT
    covmat = np.cov(meanRemoved, rowvar=0)
    print(covmat)

    # 3.对协方差矩阵做特征值分解，求得其特征值和特征向量，并将特征值从大到小排序，筛选出前topNfeat个
    eigVals, eigVects = np.linalg.eig(np.mat(covmat))
    eigValInd = np.argsort(eigVals)
    eigValInd = eigValInd[:-(topNfeat+1):-1]    # 取前topNfeat大的特征值的索引
    redEigVects = eigVects[:, eigValInd]        # 取前topNfeat大的特征值所对应的特征向量

    # 4.将数据转换到新的低维空间中
    lowDDataMat = meanRemoved * redEigVects     # 降维之后的数据
    reconMat = (lowDDataMat * redEigVects.T) + meanVals # 重构数据，可在原数据维度下进行对比查看
    return np.array(lowDDataMat), np.array(reconMat)


# ---------------------------- main ---------------------------- #

dataMat = loadDataSet('./data/testSet.txt')
lowDDataMat, reconMat = pca(dataMat, 1)
#showData(dataMat, lowDDataMat)
showData(dataMat, reconMat)
print(lowDDataMat)
```

## 5 一些对 PCA 的认知

PCA 本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。

因此，PCA 也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过 Kernel 函数将非线性相关转为线性相关。另外，PCA 假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA 的效果就大打折扣了。

最后需要说明的是，PCA 是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以 PCA 便于通用实现，但是本身无法个性化的优化。